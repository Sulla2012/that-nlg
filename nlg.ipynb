{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "narrative-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/r/rbond/jorlo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ed4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../../texts/communist-manifesto.txt').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inputs = tokenize_words(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sitting-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 48620\n",
      "Total vocab: 37\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "apart-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disturbed-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "corresponding-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 48520\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "controversial-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "banned-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.to_categorical(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "apparent-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "funky-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occupied-syndrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48520 samples\n",
      "Epoch 1/40\n",
      "48128/48520 [============================>.] - ETA: 0s - loss: 2.9529\n",
      "Epoch 00001: loss improved from inf to 2.95219, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 15s 319us/sample - loss: 2.9522\n",
      "Epoch 2/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.9117\n",
      "Epoch 00002: loss improved from 2.95219 to 2.91141, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 10s 201us/sample - loss: 2.9114\n",
      "Epoch 3/40\n",
      "48128/48520 [============================>.] - ETA: 0s - loss: 2.9046\n",
      "Epoch 00003: loss improved from 2.91141 to 2.90474, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 9s 182us/sample - loss: 2.9047\n",
      "Epoch 4/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.9018\n",
      "Epoch 00004: loss improved from 2.90474 to 2.90176, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 2.9018\n",
      "Epoch 5/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.8986\n",
      "Epoch 00005: loss improved from 2.90176 to 2.89857, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 156us/sample - loss: 2.8986\n",
      "Epoch 6/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.8988\n",
      "Epoch 00006: loss did not improve from 2.89857\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 2.8987\n",
      "Epoch 7/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.8965\n",
      "Epoch 00007: loss improved from 2.89857 to 2.89651, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 155us/sample - loss: 2.8965\n",
      "Epoch 8/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.8863\n",
      "Epoch 00008: loss improved from 2.89651 to 2.88610, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 2.8861\n",
      "Epoch 9/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.7302\n",
      "Epoch 00009: loss improved from 2.88610 to 2.73012, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 147us/sample - loss: 2.7301\n",
      "Epoch 10/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.6235\n",
      "Epoch 00010: loss improved from 2.73012 to 2.62342, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 150us/sample - loss: 2.6234\n",
      "Epoch 11/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.5092\n",
      "Epoch 00011: loss improved from 2.62342 to 2.50974, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 148us/sample - loss: 2.5097\n",
      "Epoch 12/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.4065\n",
      "Epoch 00012: loss improved from 2.50974 to 2.40612, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 152us/sample - loss: 2.4061\n",
      "Epoch 13/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.3239\n",
      "Epoch 00013: loss improved from 2.40612 to 2.32418, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 152us/sample - loss: 2.3242\n",
      "Epoch 14/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.2500\n",
      "Epoch 00014: loss improved from 2.32418 to 2.24982, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 2.2498\n",
      "Epoch 15/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.1800\n",
      "Epoch 00015: loss improved from 2.24982 to 2.18044, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 152us/sample - loss: 2.1804\n",
      "Epoch 16/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.1272\n",
      "Epoch 00016: loss improved from 2.18044 to 2.12721, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 152us/sample - loss: 2.1272\n",
      "Epoch 17/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.0755\n",
      "Epoch 00017: loss improved from 2.12721 to 2.07549, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 153us/sample - loss: 2.0755\n",
      "Epoch 18/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 2.0321\n",
      "Epoch 00018: loss improved from 2.07549 to 2.03215, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 151us/sample - loss: 2.0321\n",
      "Epoch 19/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.9894\n",
      "Epoch 00019: loss improved from 2.03215 to 1.98970, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 1.9897\n",
      "Epoch 20/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.9546\n",
      "Epoch 00020: loss improved from 1.98970 to 1.95444, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 157us/sample - loss: 1.9544\n",
      "Epoch 21/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.9270\n",
      "Epoch 00021: loss improved from 1.95444 to 1.92715, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 9s 194us/sample - loss: 1.9271\n",
      "Epoch 22/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.8979\n",
      "Epoch 00022: loss improved from 1.92715 to 1.89753, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 10s 207us/sample - loss: 1.8975\n",
      "Epoch 23/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.8680\n",
      "Epoch 00023: loss improved from 1.89753 to 1.86823, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 10s 208us/sample - loss: 1.8682\n",
      "Epoch 24/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.8423\n",
      "Epoch 00024: loss improved from 1.86823 to 1.84254, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 10s 205us/sample - loss: 1.8425\n",
      "Epoch 25/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.8220\n",
      "Epoch 00025: loss improved from 1.84254 to 1.82193, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 9s 192us/sample - loss: 1.8219\n",
      "Epoch 26/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7987\n",
      "Epoch 00026: loss improved from 1.82193 to 1.79850, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 9s 185us/sample - loss: 1.7985\n",
      "Epoch 27/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7762\n",
      "Epoch 00027: loss improved from 1.79850 to 1.77578, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 165us/sample - loss: 1.7758\n",
      "Epoch 28/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7584\n",
      "Epoch 00028: loss improved from 1.77578 to 1.75830, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 148us/sample - loss: 1.7583\n",
      "Epoch 29/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7429\n",
      "Epoch 00029: loss improved from 1.75830 to 1.74320, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 150us/sample - loss: 1.7432\n",
      "Epoch 30/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7273\n",
      "Epoch 00030: loss improved from 1.74320 to 1.72720, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 151us/sample - loss: 1.7272\n",
      "Epoch 31/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.7041\n",
      "Epoch 00031: loss improved from 1.72720 to 1.70374, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 150us/sample - loss: 1.7037\n",
      "Epoch 32/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6981\n",
      "Epoch 00032: loss improved from 1.70374 to 1.69810, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 156us/sample - loss: 1.6981\n",
      "Epoch 33/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6737\n",
      "Epoch 00033: loss improved from 1.69810 to 1.67382, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48520/48520 [==============================] - 8s 159us/sample - loss: 1.6738\n",
      "Epoch 34/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6660\n",
      "Epoch 00034: loss improved from 1.67382 to 1.66652, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 1.6665\n",
      "Epoch 35/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6484\n",
      "Epoch 00035: loss improved from 1.66652 to 1.64840, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 8s 155us/sample - loss: 1.6484\n",
      "Epoch 36/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6422\n",
      "Epoch 00036: loss improved from 1.64840 to 1.64195, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 152us/sample - loss: 1.6419\n",
      "Epoch 37/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6268\n",
      "Epoch 00037: loss improved from 1.64195 to 1.62646, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 153us/sample - loss: 1.6265\n",
      "Epoch 38/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.6092\n",
      "Epoch 00038: loss improved from 1.62646 to 1.60866, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 148us/sample - loss: 1.6087\n",
      "Epoch 39/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.5962\n",
      "Epoch 00039: loss improved from 1.60866 to 1.59601, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 1.5960\n",
      "Epoch 40/40\n",
      "48384/48520 [============================>.] - ETA: 0s - loss: 1.5905\n",
      "Epoch 00040: loss improved from 1.59601 to 1.59072, saving model to model_weights_saved.hdf5\n",
      "48520/48520 [==============================] - 7s 154us/sample - loss: 1.5907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffcb05321d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-seven",
   "metadata": {},
   "source": [
    "# TF Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "worth-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "979cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b1eb6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "948fe6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences = True, stateful = True,\n",
    "                recurrent_initializer = 'glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "animated-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../../texts/communist-manifesto.txt', 'rb').read().decode(encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "aaa721c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "879c03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7ed8ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ed535620",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "\n",
    "examples_per_epoch = len(text) // (seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a32b0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c6446b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f5df54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "dd99ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "47c8a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5ad9b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d8eadab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a28170a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'marx_{epoch}')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "aa296aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685386d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 17 steps\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 3.3517\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 2.9808\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 2.8312\n",
      "Epoch 4/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89161d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae183dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, start_string):\n",
    "    num_gen = 1000\n",
    "    start_string = 'Workers'\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_gen):\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    while idx2char[predicted_id] != '.':\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text(generative_model, 'Workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c2e3f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b691eb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ml-clusters",
   "language": "python",
   "name": "gpu-ml-clusters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
