{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premier-seven",
   "metadata": {},
   "source": [
    "# TF Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c550af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c46418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4ac0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, architecture = 'LSTM', layers = 1):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences = True, stateful = True,\n",
    "                recurrent_initializer = 'glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d37561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, EPOCHS = 100):\n",
    "    if name == 'generic':\n",
    "        text = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        text = open('../../texts/{}.txt'.format(name), 'rb').read().decode(encoding='utf-8')\n",
    "        essays = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "        text = text + essays\n",
    "    print(len(text))\n",
    "    vocab = sorted(set(text))\n",
    "    \n",
    "    char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[char] for char in text])\n",
    "    seq_len = 100\n",
    "\n",
    "    examples_per_epoch = len(text) // (seq_len + 1)\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(seq_len + 1, drop_remainder = True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    BATCH_SIZE = 64\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    embedding_dim = 256\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = loss)\n",
    "    checkpoint_path = './final_weights/{}/'.format(name)\n",
    "    try:\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('model loaded')\n",
    "    except:\n",
    "        pass\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True)\n",
    "    \n",
    "    history = model.fit(dataset, epochs = EPOCHS, callbacks = [cp_callback])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "animated-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../../texts/marx.txt', 'rb').read().decode(encoding='utf-8')\n",
    "essays = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "text = text + essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400b9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573715\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1adf001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afe6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4289f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0f9df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "\n",
    "examples_per_epoch = len(text) // (seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1caeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "623348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "333be041",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db74745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aeb913cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10890553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0add8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52dbccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573715"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cff5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_dir = './training_checkpoints'\n",
    "#checkpoint_prefix = os.path.join(checkpoint_dir, 'marx_{epoch}')\n",
    "#checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath = checkpoint_prefix, save_weights_only = True)\n",
    "checkpoint_path = './final_weights/marx/'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "615a78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97d73a18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 88 steps\n",
      "Epoch 1/100\n",
      "88/88 [==============================] - 5s 58ms/step - loss: 2.9391\n",
      "Epoch 2/100\n",
      "63/88 [====================>.........] - ETA: 1s - loss: 2.4375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c9bf4224c282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/gpu-ml-clusters/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs = EPOCHS, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457192ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0deb4113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff67cfdd190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00aaf24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e38f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, start_string):\n",
    "    num_gen = 1000\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_gen):\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    while idx2char[predicted_id] != '.':\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    ret_str = start_string + ''.join(text_generated)\n",
    "    ret_str = ret_str.replace('\\n', ' ')\n",
    "    ret_str = ret_str.split('.')\n",
    "    ret_str = '.'.join(ret_str[1:])\n",
    "    return (ret_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a2db7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(np.random.randint(0, len(text)-100))\n",
    "start_text = text[i:int(i+100)]\n",
    "generated_text = gen_text(generative_model, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88a13580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' These “Wages of Whiteness,” as Roediger described them to discover, expecial in the production of history’s direction, his abstraction is not a mutual misunderstanding of companishtof  of the divergent person and to the absolute monarchy and \"Yak at ho wine  Within Ro  some of this tas  A  discrepancy of individuals; all  they makes as he is from his fellow, although it is manipulated by his practices and are struggle into three age groups will achieved though it mythologies from the collition of bourgeois property But in the bourgeois, as well as their own cultural signifiers to gradation of cyit is self-redamental right to the proletariat are made up of time Both are es helped on diamonds and  durkhe marooon” as a multiplicity and lies him enarche’s claims, with local corresponding of society more immediately begin Spelke and Thoe chayit is not require the same abort from the first monkey recognition to fight naked by protection of its cultural outcoinsion of capital and loces that would defecting  Conditional asundesto over may have generally, as it will say, therefore, however, are not yet based on learning of an object has desireeffeoretries'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc471359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473662\n",
      "Train for 73 steps\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 4s 61ms/step - loss: 2.9812\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 2.4301\n",
      "Epoch 3/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 2.2037\n",
      "Epoch 4/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 2.0246\n",
      "Epoch 5/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.8699\n",
      "Epoch 6/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 1.7323\n",
      "Epoch 7/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.6176\n",
      "Epoch 8/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5277\n",
      "Epoch 9/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.4553\n",
      "Epoch 10/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.3983\n",
      "Epoch 11/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.3521\n",
      "Epoch 12/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.3121\n",
      "Epoch 13/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 1.2781\n",
      "Epoch 14/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.2470\n",
      "Epoch 15/100\n",
      "73/73 [==============================] - 3s 48ms/step - loss: 1.2194\n",
      "Epoch 16/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.1944\n",
      "Epoch 17/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.1708\n",
      "Epoch 18/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.1491\n",
      "Epoch 19/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.1267\n",
      "Epoch 20/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.1076\n",
      "Epoch 21/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.0862\n",
      "Epoch 22/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 1.0664\n",
      "Epoch 23/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.0480\n",
      "Epoch 24/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.0286\n",
      "Epoch 25/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.0085\n",
      "Epoch 26/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.9883\n",
      "Epoch 27/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.9706\n",
      "Epoch 28/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.9495\n",
      "Epoch 29/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.9301\n",
      "Epoch 30/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.9103\n",
      "Epoch 31/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.8910\n",
      "Epoch 32/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 0.8687\n",
      "Epoch 33/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.8489\n",
      "Epoch 34/100\n",
      "73/73 [==============================] - 3s 48ms/step - loss: 0.8281\n",
      "Epoch 35/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.8106\n",
      "Epoch 36/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.7879\n",
      "Epoch 37/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.7663\n",
      "Epoch 38/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.7488\n",
      "Epoch 39/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 0.7254\n",
      "Epoch 40/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.7091\n",
      "Epoch 41/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.6874\n",
      "Epoch 42/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.6677\n",
      "Epoch 43/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.6493\n",
      "Epoch 44/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.6303\n",
      "Epoch 45/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.6125\n",
      "Epoch 46/100\n",
      "73/73 [==============================] - 3s 43ms/step - loss: 0.5958\n",
      "Epoch 47/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.5788\n",
      "Epoch 48/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.5612\n",
      "Epoch 49/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.5435\n",
      "Epoch 50/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.5312\n",
      "Epoch 51/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.5147\n",
      "Epoch 52/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.4992\n",
      "Epoch 53/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.4877\n",
      "Epoch 54/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.4762\n",
      "Epoch 55/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.4620\n",
      "Epoch 56/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.4513\n",
      "Epoch 57/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.4405\n",
      "Epoch 58/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.4284\n",
      "Epoch 59/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.4202\n",
      "Epoch 60/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.4103\n",
      "Epoch 61/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.4011\n",
      "Epoch 62/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.3944\n",
      "Epoch 63/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3848\n",
      "Epoch 64/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3756\n",
      "Epoch 65/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.3718\n",
      "Epoch 66/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.3642\n",
      "Epoch 67/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3600\n",
      "Epoch 68/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.3524\n",
      "Epoch 69/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3474\n",
      "Epoch 70/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3412\n",
      "Epoch 71/100\n",
      "73/73 [==============================] - 3s 48ms/step - loss: 0.3371\n",
      "Epoch 72/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.3321\n",
      "Epoch 73/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.3283\n",
      "Epoch 74/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.3249\n",
      "Epoch 75/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.3185\n",
      "Epoch 76/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.3163\n",
      "Epoch 77/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3129\n",
      "Epoch 78/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3087\n",
      "Epoch 79/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.3062\n",
      "Epoch 80/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.3035\n",
      "Epoch 81/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2988\n",
      "Epoch 82/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2978\n",
      "Epoch 83/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2950\n",
      "Epoch 84/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 0.2914\n",
      "Epoch 85/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2907\n",
      "Epoch 86/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.2870\n",
      "Epoch 87/100\n",
      "73/73 [==============================] - 3s 48ms/step - loss: 0.2855\n",
      "Epoch 88/100\n",
      "73/73 [==============================] - 3s 47ms/step - loss: 0.2830\n",
      "Epoch 89/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2805\n",
      "Epoch 90/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 0.2776\n",
      "Epoch 91/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2772\n",
      "Epoch 92/100\n",
      "73/73 [==============================] - 3s 46ms/step - loss: 0.2752\n",
      "Epoch 93/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.2735\n",
      "Epoch 94/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.2707\n",
      "Epoch 95/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.2708\n",
      "Epoch 96/100\n",
      "73/73 [==============================] - 4s 48ms/step - loss: 0.2686\n",
      "Epoch 97/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.2690\n",
      "Epoch 98/100\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 0.2666\n",
      "Epoch 99/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.2640\n",
      "Epoch 100/100\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 0.2644\n"
     ]
    }
   ],
   "source": [
    "history = train_model('lenin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cef46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(np.random.randint(0, len(text)-100))\n",
    "start_text = text[i:int(i+100)]\n",
    "generated_text = gen_text(generative_model, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "542a7da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Durkheim, they, the peasant, a little do they had nothing to gain, and even the linen's own which subjects behaved either selfishled in drawing any absolute monarchy as a counterpois become more voluming the Devil, both bence the power of government, as they forgeour theoletical holding society scared of labour, so the landlord, the solid may considered useful  This property of altruism and staring and appropriating out we shall her cast, buth their philosophizing of this factions As Saci the language of colonialism In fact, we see very specifically the bourgeoisie itself, to the Deich that of the commodity considers to draws on comprehend its political power for the different kinds of commodities that the British position of the protective other in the relation of  commodity to commodity In fact, the struggle of the proletariat Racism, too, people spin and hoperty, the modern viewer reasoning information helps must focus a member or or  any ways in production More interested individuals itsplicated that something accidental and purelt involving means of production on the Jews, although quality in the Liberal tradition, as well as their status quo, one which, while this it is sed of important for providing individual motivationstomethic case study can ideologizing the Devil, both other costrainers with man is castempower, and many other linen\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2786f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_text_gen import model as generate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8078db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model('marx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21314315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e317d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = model.generate_text(num = -5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0c4155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'essary to one that things could not go on long in this regard.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710d2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a328b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ml-clusters",
   "language": "python",
   "name": "gpu-ml-clusters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
