{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premier-seven",
   "metadata": {},
   "source": [
    "# TF Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worth-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c550af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c46418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd4ac0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences = True, stateful = True,\n",
    "                recurrent_initializer = 'glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9de60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, EPOCHS = 100):\n",
    "    if name == 'generic':\n",
    "        text = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        text = open('../../texts/{}.txt'.format(name), 'rb').read().decode(encoding='utf-8')\n",
    "        essays = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "        text = text + essays\n",
    "    \n",
    "    vocab = sorted(set(text))\n",
    "    \n",
    "    char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[char] for char in text])\n",
    "    seq_len = 100\n",
    "\n",
    "    examples_per_epoch = len(text) // (seq_len + 1)\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(seq_len + 1, drop_remainder = True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    BATCH_SIZE = 64\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 256\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = loss)\n",
    "    checkpoint_path = './final_weights/{}/'.format(name)\n",
    "    try:\n",
    "        model.load_weights(checkpoint_path)\n",
    "    except:\n",
    "        pass\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True)\n",
    "    \n",
    "    history = model.fit(dataset, epochs = EPOCHS, callbacks = [cp_callback])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "animated-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../../texts/marx.txt', 'rb').read().decode(encoding='utf-8')\n",
    "essays = open('../../texts/essays.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "text = text + essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1adf001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4289f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f9df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "\n",
    "examples_per_epoch = len(text) // (seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1caeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "623348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "333be041",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db74745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb913cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10890553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0add8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccc93908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324750"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cff5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_dir = './training_checkpoints'\n",
    "#checkpoint_prefix = os.path.join(checkpoint_dir, 'marx_{epoch}')\n",
    "#checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath = checkpoint_prefix, save_weights_only = True)\n",
    "checkpoint_path = './final_weights/marx/'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "615a78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97d73a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 50 steps\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.1932\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1921\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1905\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1916\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1924\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1901\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1902\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1869\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1888\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1879\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1908\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1878\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1870\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1861\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1854\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1876\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1862\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1873\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1875\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1836\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1838\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1824\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1839\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1824\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1826\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1829\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1816\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1811\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1823\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1816\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1814\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1796\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1814\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1805\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1783\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1796\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1793\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1778\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1765\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1795\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1766\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1771\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1783\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1763\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1763\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1766\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1759\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1777\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1746\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1748\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1757\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1739\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1762\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1735\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1728\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1749\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1731\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1715\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1718\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1709\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1697\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1702\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1707\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1723\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1714\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1716\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1712\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1701\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1692\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1691\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1686\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1690\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1674\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1695\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1668\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1668\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1667\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1665\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1683\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1666\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1673\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1675\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1663\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1658\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1652\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1642\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1641\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1655\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1647\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1646\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1651\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1642\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1663\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1652\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.1648\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1640\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1649\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.1646\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.1629\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 3s 65ms/step - loss: 0.1623\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs = EPOCHS, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457192ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0deb4113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff67cfdd190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00aaf24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e38f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, start_string):\n",
    "    num_gen = 1000\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_gen):\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    while idx2char[predicted_id] != '.':\n",
    "        predictions = generative_model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    ret_str = start_string + ''.join(text_generated)\n",
    "    ret_str = ret_str.replace('\\n', ' ')\n",
    "    ret_str = ret_str.split('.')\n",
    "    ret_str = ''.join(ret_str[1:])\n",
    "    return (ret_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a2db7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(np.random.randint(0, len(text)-100))\n",
    "start_text = text[i:int(i+100)]\n",
    "generated_text = gen_text(generative_model, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88a13580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' These “Wages of Whiteness,” as Roediger described them to discover, expecial in the production of history’s direction, his abstraction is not a mutual misunderstanding of companishtof  of the divergent person and to the absolute monarchy and \"Yak at ho wine  Within Ro  some of this tas  A  discrepancy of individuals; all  they makes as he is from his fellow, although it is manipulated by his practices and are struggle into three age groups will achieved though it mythologies from the collition of bourgeois property But in the bourgeois, as well as their own cultural signifiers to gradation of cyit is self-redamental right to the proletariat are made up of time Both are es helped on diamonds and  durkhe marooon” as a multiplicity and lies him enarche’s claims, with local corresponding of society more immediately begin Spelke and Thoe chayit is not require the same abort from the first monkey recognition to fight naked by protection of its cultural outcoinsion of capital and loces that would defecting  Conditional asundesto over may have generally, as it will say, therefore, however, are not yet based on learning of an object has desireeffeoretries'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc471359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 32 steps\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 3.1233\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 2.6686\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 2.3805\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 2.2110\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 2.0676\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.9074\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.7816\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 1.6568\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.5524\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 1.4657\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 1.3911\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.3236\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 1.2658\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 1.2181\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 1.1700\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 1.1269\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 1.0853\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 1.0471\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 1.0044\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.9641\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.9258\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.8860\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.8482\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.8079\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.7654\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.7244\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.6807\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.6417\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.6006\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.5645\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.5315\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.4963\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.4677\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.4388\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.4130\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.3910\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.3696\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.3541\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.3378\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.3229\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.3111\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.2989\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 2s 47ms/step - loss: 0.2909\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.2820\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.2731\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2689\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.2626\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2549\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2501\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.2461\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2420\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2375\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2348\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.2294\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.2305\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2267\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2212\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.2211\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2179\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.2146\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2110\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.2080\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 2s 63ms/step - loss: 0.2095\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 6s 173ms/step - loss: 0.2064\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.2006\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.2000\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 0.1983\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.1964\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 2s 47ms/step - loss: 0.1935\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1949\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1928\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1915\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.1887\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1871\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1850\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.1856\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.1812\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1840\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1815\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.1808\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1783\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1776\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1754\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1764\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1752\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1741\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1725\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.1734\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1719\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1725\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.1708\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1692\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1686\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1665\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 2s 52ms/step - loss: 0.1661\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1655\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.1646\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1626\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.1634\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 52ms/step - loss: 0.1638\n"
     ]
    }
   ],
   "source": [
    "history = train_model('generic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cef46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(np.random.randint(0, len(text)-100))\n",
    "start_text = text[i:int(i+100)]\n",
    "generated_text = gen_text(generative_model, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df8b9529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Durkheim, they, the peasant, a little do they had nothing to gain, and even the linen's own which subjects behaved either selfishled in drawing any absolute monarchy as a counterpois become more voluming the Devil, both bence the power of government, as they forgeour theoletical holding society scared of labour, so the landlord, the solid may considered useful  This property of altruism and staring and appropriating out we shall her cast, buth their philosophizing of this factions As Saci the language of colonialism In fact, we see very specifically the bourgeoisie itself, to the Deich that of the commodity considers to draws on comprehend its political power for the different kinds of commodities that the British position of the protective other in the relation of  commodity to commodity In fact, the struggle of the proletariat Racism, too, people spin and hoperty, the modern viewer reasoning information helps must focus a member or or  any ways in production More interested individuals itsplicated that something accidental and purelt involving means of production on the Jews, although quality in the Liberal tradition, as well as their status quo, one which, while this it is sed of important for providing individual motivationstomethic case study can ideologizing the Devil, both other costrainers with man is castempower, and many other linen\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8023553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_text_gen import model as generate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a5996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c36ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7346c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.generate_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004de64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  He adds that the aristocracy has the purpose of breeding the next generation of rulers, as the members of this social contract for peace  Rousseau uses the term “amour propre\" to a different selection teracts that facilitate political organization lead in the eyes of the collective which feelould lead to a different rewards and from behavior and leadleisms as a whole, and not just to them as individuals This marks another person to determine the fates the selfish response to transcend the change of the colonial societies Most of them were simply trying to survive Colonial authorities to actually possess them in order to improve one’s social rank, thus making apply to form the impossible Blobs do inventive were present The inwarents of the effects various scon of history from what he believes to be the distinguishing characteristic of the human: the practical level, Fanon believe that violence is marked only themselves, and thus are apt topatable or numbers of oppression and the end of Colonialism, helping to subjugate thowever of government, one rooted in a single, inalienable interect  It is not the instrument whereby that division is brought about; it is its detes; actors of their own personhood in their attempts to achieve their goals'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736b425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-ml-clusters",
   "language": "python",
   "name": "gpu-ml-clusters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
